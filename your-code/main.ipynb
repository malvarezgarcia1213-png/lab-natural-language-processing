{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "KCk9XLsU9lJm",
        "outputId": "4b36f466-4bb6-4bf0-9b41-c47d6b9446c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbmSpCdd9lJm"
      },
      "source": [
        "# Lab | Natural Language Processing\n",
        "### SMS: SPAM or HAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBTmBMLi9lJn"
      },
      "source": [
        "### Let's prepare the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "LL472rfn9lJn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "mepMIvu9Al70",
        "outputId": "1d5ea74b-7083-4848-a194-ec33860c2069",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHJXxQwZ9lJn"
      },
      "source": [
        "- Read Data for the Fraudulent Email Kaggle Challenge\n",
        "- Reduce the training set to speead up development."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "R0cF_Gdq9lJo",
        "outputId": "0095f3f5-cf68-4586-fee6-8097c5feae31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 2)\n"
          ]
        }
      ],
      "source": [
        "## Read Data for the Fraudulent Email Kaggle Challenge\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Lab Procesing/kg_train.csv\",encoding='latin-1')\n",
        "# Reduce the training set to speed up development.\n",
        "# Modify for final system\n",
        "data = data.head(1000)\n",
        "print(data.shape)\n",
        "data.fillna(\"\",inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQgouSV89lJo"
      },
      "source": [
        "### Let's divide the training and test set into two partitions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "t4KnLP5GDzho"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)"
      ],
      "metadata": {
        "id": "WTmofC7MEQy8",
        "outputId": "30448cee-f4d1-404f-bea5-d28766744b49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['text', 'label'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "ci7CgNfL9lJo",
        "outputId": "bfe45ffa-d76f-417c-bc8c-dd8d303a6497",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: (800,)\n",
            "Test set size: (200,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "X = data[\"text\"]\n",
        "y = data[\"label\"]\n",
        "\n",
        "# Ahora divides el dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"Training set size:\", X_train.shape)\n",
        "print(\"Test set size:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ymWCMKW9lJo"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "Qe1g8ZEpEvqZ",
        "outputId": "19208bc3-5aab-45a9-b184-9971f41c899c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "85YDZlAaIcG-"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "X0Pmrrnf9lJo",
        "outputId": "5ed158b4-6f2c-4e37-b004-188c9fac733f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "print(string.punctuation)\n",
        "print(stopwords.words(\"english\")[100:110])\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "snowball = SnowballStemmer('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqyD-oPH9lJp"
      },
      "source": [
        "## Now, we have to clean the html code removing words\n",
        "\n",
        "- First we remove inline JavaScript/CSS\n",
        "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
        "- Next we can remove the remaining tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "ESF4u0KT9lJp"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_html(text):\n",
        "    # 1️⃣ Remove inline JavaScript/CSS between <script> and <style> tags\n",
        "    text = re.sub(r'<(script|style).*?>.*?(</\\1>)', '', text, flags=re.DOTALL)\n",
        "\n",
        "    # 2️⃣ Remove HTML comments (they can contain \">\")\n",
        "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
        "\n",
        "    # 3️⃣ Remove remaining HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # 4️⃣ Remove extra whitespace and newlines\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sgceSCS9lJp"
      },
      "source": [
        "- Remove all the special characters\n",
        "    \n",
        "- Remove numbers\n",
        "    \n",
        "- Remove all single characters\n",
        "\n",
        "- Remove single characters from the start\n",
        "\n",
        "- Substitute multiple spaces with single space\n",
        "\n",
        "- Remove prefixed 'b'\n",
        "\n",
        "- Convert to Lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "THQHCnpD9lJp"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    # 1️⃣ Remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # 2️⃣ Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # 3️⃣ Remove single characters\n",
        "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
        "\n",
        "    # 4️⃣ Remove single characters from the start of the text\n",
        "    text = re.sub(r'^[a-zA-Z]\\s+', '', text)\n",
        "\n",
        "    # 5️⃣ Substitute multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # 6️⃣ Remove prefixed 'b' (from byte strings)\n",
        "    text = re.sub(r'^b\\s+', '', text)\n",
        "\n",
        "    # 7️⃣ Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['clean_text'] = data['text'].apply(clean_html).apply(clean_text).apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "P6d4nmPmIK4v"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEZj4KxL9lJp"
      },
      "source": [
        "## Now let's work on removing stopwords\n",
        "Remove the stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "GaSGY8Oq9lJp"
      },
      "outputs": [],
      "source": [
        "top_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(filtered_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POciOMbb9lJp"
      },
      "source": [
        "## Tame Your Text with Lemmatization\n",
        "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "ZrBjcqPL9lJp"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def apply_lemmatization(text):\n",
        "    words = text.split()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(lemmatized_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "9hYnnynVImh4",
        "outputId": "cffd86a3-c8b5-4e35-93b7-2e789118d8d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['clean_text'] = data['clean_text'].apply(apply_lemmatization)\n"
      ],
      "metadata": {
        "id": "9zIkxVROH9oO"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DYrJioX9lJp"
      },
      "source": [
        "## Bag Of Words\n",
        "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "LG0CveTE9lJp"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ham_texts = data[data['label'] == 0]['clean_text']\n",
        "spam_texts = data[data['label'] == 1]['clean_text']\n"
      ],
      "metadata": {
        "id": "oZ1NSKBaJO9U"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(stop_words='english')\n"
      ],
      "metadata": {
        "id": "KGxNhF1rJTMG"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_ham = vectorizer.fit_transform(ham_texts)\n",
        "ham_words = pd.DataFrame({'word': vectorizer.get_feature_names_out(),\n",
        "                          'count': X_ham.sum(axis=0).A1})\n",
        "top_ham = ham_words.sort_values(by='count', ascending=False).head(10)\n",
        "print(\"Top 10 words in ham emails:\")\n",
        "print(top_ham)\n"
      ],
      "metadata": {
        "id": "cR3yhebMJTxL",
        "outputId": "392903f4-8dc4-492a-c6b7-8ed5b4b6dd53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 words in ham emails:\n",
            "           word  count\n",
            "4539         pm    115\n",
            "5710      state    103\n",
            "4691  president     94\n",
            "6160       time     84\n",
            "4443    percent     77\n",
            "5365  secretary     76\n",
            "6808       work     73\n",
            "3895         mr     71\n",
            "5269       said     62\n",
            "241    american     62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_spam = vectorizer.fit_transform(spam_texts)\n",
        "spam_words = pd.DataFrame({'word': vectorizer.get_feature_names_out(),\n",
        "                           'count': X_spam.sum(axis=0).A1})\n",
        "top_spam = spam_words.sort_values(by='count', ascending=False).head(10)\n",
        "print(\"\\nTop 10 words in spam emails:\")\n",
        "print(top_spam)\n"
      ],
      "metadata": {
        "id": "jgSHoIASJdF_",
        "outputId": "f0cbe751-3586-4012-c9a1-a03817c2299b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 words in spam emails:\n",
            "              word  count\n",
            "8196         money    920\n",
            "143        account    794\n",
            "1454          bank    745\n",
            "5399          fund    703\n",
            "2047      business    473\n",
            "12982  transaction    416\n",
            "3152       country    406\n",
            "13051     transfer    392\n",
            "8060       million    385\n",
            "2709       company    365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21CW7t-c9lJp"
      },
      "source": [
        "## Extra features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train.columns)"
      ],
      "metadata": {
        "id": "HCXdTGyqP9DJ",
        "outputId": "e42d2728-070f-4b12-c247-5c3bcda30e78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['text'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train['preprocessed_text'] = data_train['text'].str.lower().str.replace(r'\\W', ' ', regex=True)\n"
      ],
      "metadata": {
        "id": "-lG9wL7lQrAy"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "Aqw3FXVI9lJp",
        "outputId": "75ebe70d-5347-4f0d-f37b-4de2358cd264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data_val' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3482976440.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_len'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preprocessed_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdata_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'money_mark'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preprocessed_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoney_simbol_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdata_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'suspicious_words'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preprocessed_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuspicious_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdata_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_len'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preprocessed_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_val' is not defined"
          ]
        }
      ],
      "source": [
        "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
        "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
        "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
        "\n",
        "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
        "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
        "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x))\n",
        "\n",
        "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
        "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
        "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x))\n",
        "\n",
        "data_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcfHQs4I9lJq"
      },
      "source": [
        "## How would work the Bag of Words with Count Vectorizer concept?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "9TNK70Ja9lJq",
        "outputId": "2fb0c202-86af-4603-d280-3ea45d387e6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['amazing' 'is' 'love' 'nlp']\n",
            "[[0 0 1 1]\n",
            " [1 1 0 1]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"I love NLP\",\n",
        "    \"NLP is amazing\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())  # ['amazing', 'is', 'love', 'nlp']\n",
        "print(X.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpiGxYqP9lJq"
      },
      "source": [
        "## TF-IDF\n",
        "\n",
        "- Load the vectorizer\n",
        "\n",
        "- Vectorize all dataset\n",
        "\n",
        "- print the shape of the vetorized dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "E1TPID1Z9lJq"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n"
      ],
      "metadata": {
        "id": "CJmp-rjBRHdQ"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tfidf = vectorizer.fit_transform(data['clean_text'])\n"
      ],
      "metadata": {
        "id": "vh9oeH5zRK-G"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of TF-IDF matrix:\", X_tfidf.shape)\n"
      ],
      "metadata": {
        "id": "uW1Ieg5hRMii",
        "outputId": "3e1aa3e7-3ff6-4275-cb8e-3a82f0b59503",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of TF-IDF matrix: (1000, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqrkfkTl9lJq"
      },
      "source": [
        "## And the Train a Classifier?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I9n0YUqvRq3P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "rzsQg9iD9lJq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = data['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Training set:\", X_train.shape)\n",
        "print(\"Test set:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "QGpn1JoURyMo",
        "outputId": "55284860-f8e4-498a-f0e5-426a92f32fa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: (800, 5000)\n",
            "Test set: (200, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLAE-sqW9lJq"
      },
      "source": [
        "### Extra Task - Implement a SPAM/HAM classifier\n",
        "\n",
        "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
        "\n",
        "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
        "\n",
        "Your task is to **find the most relevant features**.\n",
        "\n",
        "For example, you can test the following options and check which of them performs better:\n",
        "- Using \"Bag of Words\" only\n",
        "- Using \"TF-IDF\" only\n",
        "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
        "- TF-IDF + extra flags\n",
        "\n",
        "\n",
        "You can work with teams of two persons (recommended)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0w9cwEh9lJq"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}